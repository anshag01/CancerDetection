{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Feature Extraction using RestNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T09:21:32.661460Z",
     "start_time": "2024-04-18T09:21:30.135364Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import methods \n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import sklearn.model_selection as model_selection\n",
    "import sklearn.linear_model as linear_model\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn import linear_model, metrics, model_selection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T16:43:50.968616Z",
     "start_time": "2024-04-16T16:43:50.968616Z"
    }
   },
   "outputs": [],
   "source": [
    "#Using the RestNet 50 model to extract features\n",
    "model = models.resnet50(pretrained = True)\n",
    "feature_extractor = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "feature_extractor.eval()\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data PreProcessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-16T16:43:50.969660Z"
    }
   },
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, directory, transform=None):\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "        self.images = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith((\".jpg\", \".png\"))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #Do preprocessing here \n",
    "        image_path = self.images[idx]\n",
    "        rgb_image_arr = methods.convert_rgb(image_path)\n",
    "        normalised_img = methods.z_normalization(rgb_image_arr)\n",
    "        image = Image.fromarray(normalised_img.astype('uint8'), 'RGB')\n",
    "        image_tensor = self.transform(image) if self.transform else image\n",
    "        key = os.path.basename(image_path).removesuffix('.jpg').removesuffix('.png')\n",
    "        return key, image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_folder_path =  '../dataverse_files/HAM10000_images_part_1'\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "dataset = ImageDataset(directory=original_folder_path, transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=16, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting features\n",
    "features_map2 = {}\n",
    "with torch.no_grad():\n",
    "    for batch_idx, output in enumerate(data_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images = images.cuda()\n",
    "\n",
    "        \n",
    "        key, images = output\n",
    "        batch_features = model(images) \n",
    "        #print(batch_features.shape)\n",
    "        batch_features = batch_features.view(batch_features.size(0), -1)  # Flatten features\n",
    "              \n",
    "        batch_features = batch_features.cpu().numpy()\n",
    "        \n",
    "        for i, feature in enumerate(batch_features):\n",
    "            image_id = batch_idx * data_loader.batch_size + i  # Compute global image ID/index\n",
    "            features_map2[key[i]] = feature\n",
    "            print(f'Done for image {image_id}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-16T16:43:50.974363Z"
    }
   },
   "outputs": [],
   "source": [
    "#Saving the features\n",
    "features_df = pd.DataFrame(features_map2)\n",
    "features_df\n",
    "features_df.to_json('features.json')\n",
    "features_df\n",
    "#np.save('features.npy', features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Training the model using logistic regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting model\n",
      "starting fitting\n",
      "0.811\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "scatter() missing 2 required positional arguments: 'x' and 'y'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-5-b692d9928a13>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmetrics\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0maccuracy_score\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_test\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_pred\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 29\u001B[1;33m \u001B[0mplt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mscatter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     30\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: scatter() missing 2 required positional arguments: 'x' and 'y'"
     ]
    }
   ],
   "source": [
    "features = pd.read_json('features.json')\n",
    "label = pd.read_csv('dataverse_files/HAM10000_metadata.csv')\n",
    "\n",
    "features = features.T\n",
    "label = label.set_index('image_id') #TODO: Does it make it fast or what?\n",
    "\n",
    "merged_data = features.merge(label, left_index=True, right_on='image_id') #TODO: what does this do? why can't we switch features and label\n",
    "\n",
    "merged_data['cancer'] = False\n",
    "cancerous = [\"akiec\", \"bcc\", \"mel\"]\n",
    "non_cancerous = [\"bkl\", \"df\", \"nv\", \"vasc\"]\n",
    "merged_data.loc[merged_data['dx'].isin(cancerous), \"cancer\"] = True\n",
    "merged_data.loc[merged_data['dx'].isin(non_cancerous), \"cancer\"] = False\n",
    "merged_data.drop(columns=['lesion_id', 'dx_type','age','sex','localization','dataset', 'dx'], inplace=True)\n",
    "\n",
    "x = merged_data.drop(columns=['cancer'], axis=1)\n",
    "y = merged_data['cancer']\n",
    "\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"starting model\")\n",
    "model = linear_model.LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=10000)\n",
    "print(\"starting fitting\")\n",
    "model.fit(x_train, y_train) #TODO: which model to pick and how many cores to run on?\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# plt.scatter()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T09:25:11.185274Z",
     "start_time": "2024-04-18T09:24:59.130267Z"
    }
   },
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
